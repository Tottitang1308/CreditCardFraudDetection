{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Load dataset from OpenML\n",
    "data = fetch_openml('creditcard', version=1, as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Display the first five rows to get a sense of the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Credit Card Dataset:\n",
    "The creditcard dataset contains transactions made by credit cards. Each transaction is a row in the dataset, and the details of each transaction are in the columns. In the context of this dataset, the crucial piece of information we want to find out is whether a transaction is fraudulent or not.\n",
    "\n",
    "### What are the V1-V28 Features?\n",
    "In the dataset, you see columns labeled V1 through V28. These aren't the actual, raw data from the credit card transactions. Instead, they've been transformed through a process called PCA, which stands for Principal Component Analysis. Here's what that means:\n",
    "\n",
    "1. Privacy Concerns: Credit card transaction data is sensitive because it contains personal information. To protect the privacy of individuals, it’s crucial to anonymize the data before it can be used for analysis.\n",
    "2. Dimensionality Reduction: The original data might have had a lot of different features (like where the purchase was made, the time, the amount, etc.), which could be challenging to work with and might not all be equally important.\n",
    "3. Principal Component Analysis (PCA): PCA is a statistical technique that helps in such situations. It takes all the original features and transforms them into a new set of features. These new features (the V columns in your dataset) are the \"principal components\" that are generated in such a way that the first few retain most of the important information from all the original features. They're constructed to be independent of one another, which is a nice property that often makes models work better.\n",
    "\n",
    "### How Do the V1-V28 Features Link to the 'Amount'?\n",
    "The Amount column in the dataset represents the transaction amount, which is a direct figure — how much money was involved in the transaction. Unlike the V1-V28 features, Amount hasn’t been transformed through PCA and is the actual, raw data.\n",
    "\n",
    "The V1-V28 features and the Amount are related only in the sense that they all describe aspects of the same transaction. The V features capture the underlying patterns and characteristics of the transaction (minus the amount), while the Amount tells us its monetary value. In fraud detection, it’s useful to look at both these sets of information because both the transaction’s characteristics and the amount could potentially be indicators of fraudulent activity.\n",
    "\n",
    "### Why is PCA Used?\n",
    "- Anonymization: It hides any sensitive information contained in the original features.\n",
    "- Efficiency: It reduces the number of features, making computations more manageable, especially if there were hundreds or thousands of original features.\n",
    "- Performance: By focusing on the most informative patterns, it can actually help machine learning models perform better by emphasizing the most significant signals in the data.\n",
    "\n",
    "\n",
    "So, in summary, when you’re working with the creditcard dataset, the V1-V28 features give you a processed, privacy-safe, and compact representation of the original transaction data, while the Amount gives you the transaction’s monetary value. Together, these pieces of information are used to determine whether a transaction is likely to be fraudulent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "0    99.827251\n",
      "1     0.172749\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking the balance of classes in your target variable, Class. \n",
    "# Crucial step before proceeding with building a machine learning model, especially with algorithms like RandomForestClassifier. \n",
    "class_counts = df['Class'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Calculating the percentage of each class\n",
    "class_percentages = class_counts / len(df) * 100\n",
    "print(class_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation: Load Dataset Step\n",
    "#### Observation:\n",
    "Upon loading the creditcard dataset from the OpenML repository, the following observations can be made:\n",
    "\n",
    "- The dataset consists of transactions made by credit cards, where each row represents a transaction.\n",
    "- There are 30 columns in total, with 28 being the result of a PCA transformation (V1 to V28), which anonymizes and reduces the dimensionality of the original features while retaining the essential information.\n",
    "- The Amount column represents the transaction amount. This feature is not normalized.\n",
    "\n",
    "The Class column is the response variable, indicating whether the transaction was fraudulent (1) or not (0).\n",
    "The dataset provides an excellent example of real-world data where dimensionality reduction has already been applied to preserve user privacy. It's imbalanced, which is common in fraud detection scenarios, where fraudulent transactions are rarer than legitimate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.694242</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>1.672773</td>\n",
       "      <td>0.973366</td>\n",
       "      <td>-0.245117</td>\n",
       "      <td>0.347068</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.082637</td>\n",
       "      <td>0.331128</td>\n",
       "      <td>0.083386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326118</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.382854</td>\n",
       "      <td>-0.176911</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>0.246585</td>\n",
       "      <td>-0.392170</td>\n",
       "      <td>0.330892</td>\n",
       "      <td>-0.063781</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.608496</td>\n",
       "      <td>0.161176</td>\n",
       "      <td>0.109797</td>\n",
       "      <td>0.316523</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>-0.061820</td>\n",
       "      <td>-0.063700</td>\n",
       "      <td>0.071253</td>\n",
       "      <td>-0.232494</td>\n",
       "      <td>-0.153350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089611</td>\n",
       "      <td>-0.307377</td>\n",
       "      <td>-0.880077</td>\n",
       "      <td>0.162201</td>\n",
       "      <td>-0.561131</td>\n",
       "      <td>0.320694</td>\n",
       "      <td>0.261069</td>\n",
       "      <td>-0.022256</td>\n",
       "      <td>0.044608</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.693500</td>\n",
       "      <td>-0.811578</td>\n",
       "      <td>1.169468</td>\n",
       "      <td>0.268231</td>\n",
       "      <td>-0.364572</td>\n",
       "      <td>1.351454</td>\n",
       "      <td>0.639776</td>\n",
       "      <td>0.207373</td>\n",
       "      <td>-1.378675</td>\n",
       "      <td>0.190700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680975</td>\n",
       "      <td>0.337632</td>\n",
       "      <td>1.063358</td>\n",
       "      <td>1.456320</td>\n",
       "      <td>-1.138092</td>\n",
       "      <td>-0.628537</td>\n",
       "      <td>-0.288447</td>\n",
       "      <td>-0.137137</td>\n",
       "      <td>-0.181021</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.493325</td>\n",
       "      <td>-0.112169</td>\n",
       "      <td>1.182516</td>\n",
       "      <td>-0.609727</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>0.936150</td>\n",
       "      <td>0.192071</td>\n",
       "      <td>0.316018</td>\n",
       "      <td>-1.262503</td>\n",
       "      <td>-0.050468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269855</td>\n",
       "      <td>-0.147443</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>-0.304777</td>\n",
       "      <td>-1.941027</td>\n",
       "      <td>1.241904</td>\n",
       "      <td>-0.460217</td>\n",
       "      <td>0.155396</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.591330</td>\n",
       "      <td>0.531541</td>\n",
       "      <td>1.021412</td>\n",
       "      <td>0.284655</td>\n",
       "      <td>-0.295015</td>\n",
       "      <td>0.071999</td>\n",
       "      <td>0.479302</td>\n",
       "      <td>-0.226510</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>0.691625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529939</td>\n",
       "      <td>-0.012839</td>\n",
       "      <td>1.100011</td>\n",
       "      <td>-0.220123</td>\n",
       "      <td>0.233250</td>\n",
       "      <td>-0.395202</td>\n",
       "      <td>1.041611</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.651816</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.694242 -0.044075  1.672773  0.973366 -0.245117  0.347068  0.193679   \n",
       "1  0.608496  0.161176  0.109797  0.316523  0.043483 -0.061820 -0.063700   \n",
       "2 -0.693500 -0.811578  1.169468  0.268231 -0.364572  1.351454  0.639776   \n",
       "3 -0.493325 -0.112169  1.182516 -0.609727 -0.007469  0.936150  0.192071   \n",
       "4 -0.591330  0.531541  1.021412  0.284655 -0.295015  0.071999  0.479302   \n",
       "\n",
       "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0  0.082637  0.331128  0.083386  ...  0.326118 -0.024923  0.382854 -0.176911   \n",
       "1  0.071253 -0.232494 -0.153350  ... -0.089611 -0.307377 -0.880077  0.162201   \n",
       "2  0.207373 -1.378675  0.190700  ...  0.680975  0.337632  1.063358  1.456320   \n",
       "3  0.316018 -1.262503 -0.050468  ... -0.269855 -0.147443  0.007267 -0.304777   \n",
       "4 -0.226510  0.744326  0.691625  ...  0.529939 -0.012839  1.100011 -0.220123   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  \n",
       "0  0.110507  0.246585 -0.392170  0.330892 -0.063781  0.244964  \n",
       "1 -0.561131  0.320694  0.261069 -0.022256  0.044608 -0.342475  \n",
       "2 -1.138092 -0.628537 -0.288447 -0.137137 -0.181021  1.160686  \n",
       "3 -1.941027  1.241904 -0.460217  0.155396  0.186189  0.140534  \n",
       "4  0.233250 -0.395202  1.041611  0.543620  0.651816 -0.073403  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Preprocess Data\n",
    "# Drop the 'Class' column to isolate the features\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# Isolate the 'Class' column to use as the target variable\n",
    "y = df['Class']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features, not including the target variable\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame for better readability\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Display the first five rows of the scaled features\n",
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation: Preprocess Data Step\n",
    "#### Observation:\n",
    "The raw dataset requires preprocessing before it can be fed into a machine learning model:\n",
    "\n",
    "- The features V1 to V28 are already scaled as a result of PCA, but the Amount feature is not. It varies significantly in magnitude which could potentially bias a machine learning algorithm.\n",
    "- The Class variable is the target for our predictive modeling task. It's separate from the input features.\n",
    "\n",
    "#### Why Preprocessing:\n",
    "Preprocessing is a critical step to ensure that the machine learning model receives the data in a suitable format, improving its ability to learn and make predictions. Specifically:\n",
    "\n",
    "- Feature Scaling: Many machine learning algorithms perform better when numerical input variables are scaled to a standard range. StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
    "- Separating Features and Target: We need to separate the input features (X) from the target variable (y) as they serve different purposes in model training and evaluation.\n",
    "\n",
    "#### What Kind of Preprocessing:\n",
    "\n",
    "- Standard Scaling: The Amount variable will be scaled using StandardScaler to ensure that all features contribute equally to the result.\n",
    "- Feature-Target Split: The dataset is split into input features (X) and the target variable (y), where X contains all columns except for Class, and y is just the Class column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 227845, Test set size: 56962\n"
     ]
    }
   ],
   "source": [
    "#4. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "#5. Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation: Train Model Step\n",
    "#### Objective:\n",
    "To develop a predictive model capable of distinguishing between legitimate and fraudulent transactions.\n",
    "\n",
    "#### Model Selection:\n",
    "Random Forest Classifier was chosen due to its proficiency in handling imbalanced datasets, like our credit card transactions, where fraudulent cases are much fewer than legitimate ones.\n",
    "\n",
    "#### Training Process:\n",
    "\n",
    "- Random Forest: An ensemble learning method that operates by constructing a multitude of decision trees. It outputs the mode of the classes (classification) of the individual trees.\n",
    "- Number of Estimators: Set to 100, implying that the model uses 100 individual decision trees to make its predictions, a number chosen to balance between computational efficiency and model performance.\n",
    "- Random State: Set to 42 to ensure reproducibility of the results; the same random seed allows the model to produce the same results on each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995611109160493\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.97      0.77      0.86        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.99      0.88      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation: Evaluate Model Step\n",
    "#### Objective:\n",
    "To assess the trained model's ability to accurately predict fraudulent transactions on unseen data.\n",
    "\n",
    "#### Evaluation Metrics:\n",
    "\n",
    "- Accuracy: Measures the proportion of true results among the total number of cases examined. Given the high score, the model is highly accurate overall.\n",
    "- Precision (for Class 1): Indicates the proportion of positive identifications that were actually correct; a precision of 0.97 means that when the model predicts a transaction is fraudulent, it is correct 97% of the time.\n",
    "- Recall (for Class 1): Describes the model's ability to capture actual positive cases; a recall of 0.77 for the fraudulent transactions implies the model correctly identifies 77% of all fraudulent transactions.\n",
    "- F1-Score (for Class 1): Combines precision and recall into a single metric, which is particularly useful for imbalanced classes; an F1-score of 0.86 is quite high, indicating a good balance between precision and recall.\n",
    "- Support: The number of actual occurrences of each class in the specified dataset; for instance, there were 98 actual fraudulent transactions.\n",
    "\n",
    "#### Observation:\n",
    "The model exhibits excellent overall performance, but given the context of fraud detection, the focus should especially be on the recall for Class 1, which could be improved. While the accuracy is exceptionally high, this metric can be misleading in imbalanced datasets where the majority class (legitimate transactions) dominates. Therefore, precision, recall, and the F1-score for the fraudulent class (Class 1) are more crucial indicators of performance.\n",
    "\n",
    "#### Insights:\n",
    "The relatively lower recall for Class 1 suggests room for improvement, perhaps by tuning hyperparameters, gathering more diverse training data, or trying alternative anomaly detection techniques. It's critical to improve recall for Class 1 in fraud detection scenarios to ensure that fraudulent transactions are not missed, as the cost of missing a fraudulent transaction can be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific terms of a confusion matrix are:\n",
    "\n",
    "- True Positives (TP): The cases in which the model correctly predicted the positive class.\n",
    "- True Negatives (TN): The cases in which the model correctly predicted the negative class.\n",
    "- False Positives (FP): The cases in which the model incorrectly predicted the positive class.\n",
    "- False Negatives (FN): The cases in which the model incorrectly predicted the negative class.\n",
    "\n",
    "Based on the layout of the confusion matrix (which typically follows the format of [[TN, FP], [FN, TP]]):\n",
    "\n",
    "- Top-Left (TN): 56862 - The number of true negatives, i.e., the model correctly predicted the negative class 56,862 times.\n",
    "- Top-Right (FP): 2 - The number of false positives, i.e., the model incorrectly predicted the positive class 2 times.\n",
    "- Bottom-Left (FN): 23 - The number of false negatives, i.e., the model incorrectly predicted the negative class 23 times.\n",
    "- Bottom-Right (TP): 75 - The number of true positives, i.e., the model correctly predicted the positive class 75 times.\n",
    "\n",
    "In the context of fraud detection:\n",
    "\n",
    "- True Positives (75) are the transactions that were correctly identified as fraudulent.\n",
    "- True Negatives (56862) are the transactions that were correctly identified as not fraudulent.\n",
    "- False Positives (2) are the transactions that were incorrectly flagged as fraudulent (also known as Type I error).\n",
    "- False Negatives (23) are the fraudulent transactions that the model failed to identify (also known as Type II error).\n",
    "- The effectiveness of the model can be interpreted as follows:\n",
    "\n",
    "The model is quite good at identifying non-fraudulent transactions, as indicated by the high number of true negatives.\n",
    "- The model has a low number of false positives, which is good because it means that there are very few non-fraudulent transactions incorrectly labeled as fraudulent.\n",
    "- The number of false negatives, although relatively small, is critical in a fraud detection context since these represent fraudulent transactions that were not detected by the model. Depending on the cost of fraud versus the cost of intervention, this number might be a concern.\n",
    "- The true positives are the actual fraudulent transactions that the model has successfully identified, which is the primary goal of a fraud detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "Given the context of the creditcard dataset which is highly imbalanced, we'll focus on hyperparameter tuning and exploring model-specific settings that can help improve recall for the fraudulent class (Class 1).\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "We will use GridSearchCV from scikit-learn to search for the best hyperparameters. This function performs an exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "The parameters we will tune are:\n",
    "\n",
    "- n_estimators: The number of trees in the forest.\n",
    "- max_features: The number of features to consider when looking for the best split.\n",
    "- max_depth: The maximum depth of the tree.\n",
    "- class_weight: Weights associated with classes. In our case, we will adjust this to deal with the imbalance by giving more weight to the minority class.\n",
    "\n",
    "### Documentation: Hyperparameter Tuning Step\n",
    "#### Objective:\n",
    "To improve the model's recall for fraudulent transactions (Class 1) by finding the optimal set of hyperparameters.\n",
    "\n",
    "#### Tuning Process:\n",
    "\n",
    "- GridSearchCV: A systematic approach to tuning hyperparameters through cross-validation. It evaluates and compares all the possible combinations of hyperparameter values provided.\n",
    "- Cross-Validation: It divides the dataset into k folds, using k-1 folds for training and the remaining fold for testing, cycling through all folds. This helps ensure that the improvement is consistent across different subsets of the data.\n",
    "- Scoring Metric: The scoring parameter is set to 'recall' to emphasize the importance of correctly identifying fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layman Term explanation:\n",
    "1. Hyperparameters: These are the settings that govern the overall behavior of a machine learning model. Think of them as the dials and knobs that you can tweak to optimize how the model learns from data. Unlike model parameters that are learned automatically from the data, hyperparameters are set by the data scientist before training begins.\n",
    "2. Random Forest Classifier: Imagine you’re not sure about what to eat for dinner and you ask your friends. Each friend gives you their suggestion. In the end, you choose the option that the majority of your friends agree on. A Random Forest works similarly by combining the decisions of many individual decision trees to make a final prediction.\n",
    "3. GridSearchCV: This is like holding a series of auditions to find the best settings for our model. It tries out different combinations of the hyperparameters you’ve specified (the number of trees, the maximum depth of each tree, etc.) to see which combination gives the best performance. It does this through a process called cross-validation:\n",
    "- Cross-validation: Instead of using the whole dataset to train the model at once, the dataset is split into smaller parts (folds). The model is trained on some parts and validated on the others, several times over. This helps to make sure that our model’s performance is stable across different subsets of the data.\n",
    "4. Scoring 'recall': Since we are specifically interested in catching as many fraudulent transactions as possible (even if we catch some non-fraudulent ones in the net), we focus on improving \"recall\". Recall is the ability of a model to find all the relevant cases within a dataset. In fraud detection, it would be the ability of our model to catch all the fraudulent transactions.\n",
    "5. n_jobs=-1: This simply tells the computer to use all its available processing power to run these auditions in parallel, which speeds up the process.\n",
    "\n",
    "The goal of this entire procedure is to improve the model’s ability to correctly identify fraudulent transactions (we want high \"recall\"). Once we find the best settings (best_params), we use them to make our model the best fraud detector it can be, then we test it one more time on our testing set to see how well it performs with these optimized settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "Unique classes in y_train: [0 1]\n",
      "int32\n",
      "Unique classes in y_test: [0 1]\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train to integer type\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Check the data type of y_train again to ensure it's now integer\n",
    "print(y_train.dtype)\n",
    "\n",
    "# Confirm the unique classes in y_train are integers\n",
    "unique_classes = np.unique(y_train)\n",
    "print(\"Unique classes in y_train:\", unique_classes)\n",
    "\n",
    "# Convert y_test to integer type\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Check the data type of y_test again to ensure it's now integer\n",
    "print(y_test.dtype)\n",
    "\n",
    "# Confirm the unique classes in y_test are integers\n",
    "unique_classes = np.unique(y_test)\n",
    "print(\"Unique classes in y_test:\", unique_classes)\n",
    "\n",
    "print(predictions.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 43.2min\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed: 51.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'class_weight': {0: 1, 1: 100}, 'max_depth': 10, 'max_features': 'auto', 'n_estimators': 150}\n",
      "Best recall: 0.7715899452540674\n",
      "Classification Report for the best estimator:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.90      0.80      0.84        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.95      0.90      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a parameter grid to search for the best parameters\n",
    "# Make sure all values are numeric\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [10, 20, None],  # None is acceptable as a value indicating no limit\n",
    "    'class_weight': [{0: 1, 1: v} for v in [1, 10, 100]]  # Ensure these are all numeric\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "cv = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='recall', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best recall\n",
    "best_params = cv.best_params_\n",
    "best_recall = cv.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best recall: {best_recall}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for the best estimator:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.90      0.80      0.84        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.95      0.90      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with the best estimator\n",
    "best_rf = cv.best_estimator_\n",
    "best_predictions = best_rf.predict(X_test)\n",
    "\n",
    "best_rf\n",
    "\n",
    "print(\"Classification Report for the best estimator:\\n\", classification_report(y_test, best_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              V1        V2        V3        V4        V5        V6        V7  \\\n",
      "541    -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545 -2.537387   \n",
      "6108   -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536 -3.496197   \n",
      "6331    0.008430  4.137837 -6.240697  6.675732  0.768307 -3.353060 -1.631735   \n",
      "6334    0.026779  4.132464 -6.560600  6.348557  1.329666 -2.513479 -1.689102   \n",
      "6336    0.329594  3.712889 -5.775935  6.078266  1.667359 -2.420168 -0.812891   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "276864 -1.374424  2.793185 -4.346572  2.400731 -1.688433  0.111136 -0.922038   \n",
      "279863 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494 -0.882850   \n",
      "280143  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536 -1.413170   \n",
      "280149 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346 -2.234739   \n",
      "281144 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548 -2.208002   \n",
      "\n",
      "              V8        V9       V10  ...       V22       V23       V24  \\\n",
      "541     1.391657 -2.770089 -2.772272  ... -0.035049 -0.465211  0.320198   \n",
      "6108   -0.248778 -0.247768 -4.801637  ...  0.176968 -0.436207 -0.053502   \n",
      "6331    0.154612 -2.795892 -6.187891  ... -0.608057 -0.539528  0.128940   \n",
      "6334    0.303253 -3.139409 -6.045468  ... -0.576752 -0.669605 -0.759908   \n",
      "6336    0.133080 -2.214311 -5.134454  ... -0.652450 -0.551572 -0.716522   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "276864 -2.149930 -2.027474 -4.390842  ...  0.504849  0.137994  0.368275   \n",
      "279863  0.697211 -2.064945 -5.587794  ... -0.319189  0.639419 -0.294885   \n",
      "280143  0.248525 -1.127396 -3.232153  ...  0.028234 -0.145640 -0.081049   \n",
      "280149  1.210158 -0.652250 -3.463891  ...  0.834108  0.190944  0.032070   \n",
      "281144  1.058733 -1.632333 -5.245984  ... -0.269209 -0.456108 -0.183659   \n",
      "\n",
      "             V25       V26       V27       V28  Amount  Class  Predicted_Class  \n",
      "541     0.044519  0.177840  0.261145 -0.143276    0.00      1                1  \n",
      "6108    0.252405 -0.657488 -0.827136  0.849573   59.00      1                1  \n",
      "6331    1.488481  0.507963  0.735822  0.513574    1.00      1                1  \n",
      "6334    1.605056  0.540675  0.737040  0.496699    1.00      1                1  \n",
      "6336    1.415717  0.555265  0.530507  0.404474    1.00      1                1  \n",
      "...          ...       ...       ...       ...     ...    ...              ...  \n",
      "276864  0.103137 -0.414209  0.454982  0.096711  349.08      1                1  \n",
      "279863  0.537503  0.788395  0.292680  0.147968  390.00      1                1  \n",
      "280143  0.521875  0.739467  0.389152  0.186637    0.76      1                1  \n",
      "280149 -0.739695  0.471111  0.385107  0.194361   77.89      1                1  \n",
      "281144 -0.328168  0.606116  0.884876 -0.253700  245.00      1                1  \n",
      "\n",
      "[409 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to predict\n",
    "new_predictions = best_rf.predict(X)\n",
    "\n",
    "# Add the predictions back to the original DataFrame\n",
    "df['Predicted_Class'] = new_predictions\n",
    "\n",
    "# Filter to get only the fraudulent transactions\n",
    "fraudulent_transactions = df[df['Predicted_Class'] == 1]\n",
    "\n",
    "# Display fraudulent transactions\n",
    "print(fraudulent_transactions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
